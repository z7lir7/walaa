{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71898f5a-882e-4205-9d0e-f7a59c07948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a2e857-86f7-448e-bc47-7feb8c59146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccfd14c-7a0b-4d79-8223-b0854d9ec960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تحميل الموارد المطلوبة\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c443868-549b-4470-8ad7-3293139df363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     mutant book wins guardian prize a book about t...   \n",
      "1     running around the olympics it was back to off...   \n",
      "2     parker misses england clash tom shanklin will ...   \n",
      "3     munster cup tie switched to spain munster s he...   \n",
      "4     cyber crime booms in 2004 the last 12 months h...   \n",
      "...                                                 ...   \n",
      "2220  smart search lets art fans browse if you don t...   \n",
      "2221  clijsters hope on aussie open kim clijsters ha...   \n",
      "2222  mps quiz aides over royal income senior offici...   \n",
      "2223  podcasters  look to net money nasa is doing it...   \n",
      "2224  bargain calls widen softbank loss japanese com...   \n",
      "\n",
      "                                           final_result  \n",
      "0     mutant book win guardian prize book evolut mut...  \n",
      "1     run around olymp back offici duti last week ro...  \n",
      "2     parker miss england clash tom shanklin start c...  \n",
      "3     munster cup tie switch spain munster heineken ...  \n",
      "4     cyber crime boom 2004 last 12 month seen drama...  \n",
      "...                                                 ...  \n",
      "2220  smart search let art fan brows know art know l...  \n",
      "2221  clijster hope aussi open kim clijster deni rep...  \n",
      "2222  mp quiz aid royal incom senior offici two bodi...  \n",
      "2223  podcast look net money nasa 14yearold boy bedr...  \n",
      "2224  bargain call widen softbank loss japanes commu...  \n",
      "\n",
      "[2225 rows x 2 columns]\n",
      "تم حفظ ميزات Bag of Words وTF-IDF في الملف: C:\\Users\\hp\\Desktop\\final.xlsx\n",
      "SVM Accuracy: 0.17303370786516853\n",
      "Naive Bayes Accuracy: 0.19775280898876405\n",
      "Random Forest Accuracy: 0.1842696629213483\n",
      "\n",
      "SVM Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.23      0.21      0.22       120\n",
      "entertainment       0.08      0.06      0.07        71\n",
      "     politics       0.17      0.17      0.17        75\n",
      "        sport       0.18      0.29      0.22        96\n",
      "         tech       0.14      0.08      0.10        83\n",
      "\n",
      "     accuracy                           0.17       445\n",
      "    macro avg       0.16      0.16      0.16       445\n",
      " weighted avg       0.17      0.17      0.17       445\n",
      "\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.22      0.26      0.24       120\n",
      "entertainment       0.12      0.03      0.05        71\n",
      "     politics       0.17      0.13      0.15        75\n",
      "        sport       0.21      0.45      0.28        96\n",
      "         tech       0.08      0.02      0.04        83\n",
      "\n",
      "     accuracy                           0.20       445\n",
      "    macro avg       0.16      0.18      0.15       445\n",
      " weighted avg       0.17      0.20      0.17       445\n",
      "\n",
      "\n",
      "Random Forest Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.24      0.28      0.26       120\n",
      "entertainment       0.08      0.06      0.07        71\n",
      "     politics       0.13      0.09      0.11        75\n",
      "        sport       0.19      0.31      0.24        96\n",
      "         tech       0.19      0.10      0.13        83\n",
      "\n",
      "     accuracy                           0.18       445\n",
      "    macro avg       0.16      0.17      0.16       445\n",
      " weighted avg       0.17      0.18      0.17       445\n",
      "\n",
      "حجم مجموعة التدريب: 1780\n",
      "حجم مجموعة الاختبار: 445\n"
     ]
    }
   ],
   "source": [
    "# إعداد الستيمينغ والكلمات الشائعة\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))  # يمكنك تغيير اللغة حسب الحاجة\n",
    "\n",
    "def clean_text(text):\n",
    "    # إزالة الأحرف غير المرغوب فيها\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # تحويل النص إلى حروف صغيرة\n",
    "    text = text.lower()\n",
    "    \n",
    "    # تقسيم النص إلى كلمات\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # إزالة الكلمات الشائعة\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # تطبيق الستيمينغ\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# قراءة البيانات من ملف Excel\n",
    "file_path = r'C:\\Users\\hp\\Desktop\\‏‏‏‏BBC news.xlsx'  # تأكد من تعديل المسار حسب الحاجة\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# افترض أن العمود الذي يحتوي على النص هو \"text\"\n",
    "data['final_result'] = data['text'].apply(clean_text)\n",
    "\n",
    "# عرض النتائج\n",
    "print(data[['text', 'final_result']])\n",
    "\n",
    "# استخراج الميزات باستخدام Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=1000)  # يمكنك تعديل العدد حسب الحاجة\n",
    "bow_matrix = bow_vectorizer.fit_transform(data['final_result'])\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# استخراج الميزات باستخدام TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # يمكنك تعديل العدد حسب الحاجة\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['final_result'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# دمج الميزات في DataFrame واحد\n",
    "combined_df = pd.concat([data[['final_result']], bow_df, tfidf_df], axis=1)\n",
    "\n",
    "# حفظ الميزات في ملف Excel واحد\n",
    "output_file = r'C:\\Users\\hp\\Desktop\\final.xlsx'  # تأكد من تعديل المسار حسب الحاجة\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "print(\"تم حفظ ميزات Bag of Words وTF-IDF في الملف:\", output_file)\n",
    "\n",
    "# إعداد المتغيرات للنموذج\n",
    "X = data['final_result']  # النصوص\n",
    "y = data['category']  # التسميات (تأكد من وجود عمود التسميات)\n",
    "\n",
    "# تقسيم البيانات إلى مجموعة تدريب ومجموعة اختبار\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# تحويل النصوص إلى ميزات باستخدام TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# تصنيف باستخدام SVM\n",
    "svm_model = SVC(kernel='linear')  # يمكنك تعديل النواة حسب الحاجة\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# تصنيف باستخدام Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# تصنيف باستخدام Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# تقييم النماذج\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_predictions))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
    "\n",
    "# طباعة تقرير التصنيف\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_predictions))\n",
    "print(\"\\nNaive Bayes Classification Report:\\n\", classification_report(y_test, nb_predictions))\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_predictions))\n",
    "\n",
    "# عرض أحجام المجموعات\n",
    "print(\"حجم مجموعة التدريب:\", X_train.shape[0])\n",
    "print(\"حجم مجموعة الاختبار:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2c808-0e08-41e5-b8ea-43dadda30f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
